{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import median, diff\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_back_included = 8\n",
    "num_classes = 5\n",
    "num_features = 40\n",
    "num_features_total = (num_features * samples_back_included) + 4\n",
    "save_files = listdir('data')\n",
    "\n",
    "def get_features_for_index(beat_features, notes, index):\n",
    "    if index < 0:\n",
    "        return [0] * num_features\n",
    "    return beat_features[index]\n",
    "\n",
    "def get_class_for_index(notes, index):\n",
    "    if index < 0:\n",
    "        return (0, 0)\n",
    "    return notes[index][0].count('1')\n",
    "    \n",
    "importance_rankings = [48, 24, 12, 16, 6, 8, 3, 4, 2, 1]\n",
    "def get_beat_importance(index):\n",
    "    for i in range(len(importance_rankings)):\n",
    "        if index % importance_rankings[i] == 0:\n",
    "            return i\n",
    "\n",
    "def get_features_for_song(key, is_full):\n",
    "    X = []\n",
    "    y = []\n",
    "    if '{0}_beat_features.csv'.format(key) in save_files and '{0}_notes.csv'.format(key) in save_files:\n",
    "        beat_features_rotated = pd.read_csv('data/{0}_beat_features.csv'.format(key)).values\n",
    "        notes = pd.read_csv('data/{0}_notes.csv'.format(key), converters={'0': lambda x: str(x)}).values\n",
    "        beat_features = np.flipud(np.rot90(np.array(beat_features_rotated)))\n",
    "        num_notes = min(len(notes), len(beat_features))\n",
    "        for i in range(num_notes):\n",
    "            row_y = get_class_for_index(notes, i)\n",
    "            if is_full or (not (row_y == 0 and random.randint(0, 20) != 0) and not (row_y == 1 and random.randint(0, 3) != 0)):\n",
    "                features = [feature for j in range(samples_back_included) for feature in get_features_for_index(beat_features, notes, i - j)]\n",
    "                features.append(i % 48)\n",
    "                features.append(get_beat_importance(i))\n",
    "                features.append(i / 48)\n",
    "                features.append(num_notes - i / 48)\n",
    "                X.append(features)\n",
    "                y.append(row_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_batch_generator():\n",
    "    songs_to_use = pd.read_csv('data/songs_to_use.csv').values\n",
    "    for song_data in songs_to_use:\n",
    "        yield (get_features_for_song(song_data[0]))\n",
    "\n",
    "# Total 243 songs\n",
    "def build_training_data(songs_start, songs_end, is_full = False):\n",
    "    X = []\n",
    "    y = []\n",
    "    songs_to_use = pd.read_csv('data/songs_to_use.csv').values\n",
    "    for song_data in songs_to_use[songs_start:songs_end]:\n",
    "        song_X, song_y = get_features_for_song(song_data[0], is_full)\n",
    "        X.extend(song_X)\n",
    "        y.extend(song_y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start with each input secion maps to one note\n",
    "# train model for that (just list comprhension on noets for contains 1 maps to true)\n",
    "\n",
    "# then move to bars eg section of 4 bars maps to output for each note\n",
    "# error = probability of note being true vs was it really\n",
    "\n",
    "# try bar + prev notes (home use weird dimensioned data?) to predict next notes\n",
    "\n",
    "# try feeding in non structured data (bpm, position of time in song, song length, \n",
    "# things about feel of song (generated features))\n",
    "# try using keras merge layer to add extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make two separate classifiers, one for note importance, then another to take importance and previous notes/importance and \n",
    "# output the notes\n",
    "\n",
    "# can either go back and include prev samples to get importance or atual notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try building one network to output importance of beats, another to output song from that and prev notes\n",
    "# Also start building sequentially not off of stepfile and test on new songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = build_training_data(0, 243)\n",
    "#X_test, y_test = build_training_data(200, 243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_full, y_train_full = build_training_data(0, 243, True)\n",
    "X_test_full, y_test_full = build_training_data(200, 243, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beat_feature_model = Sequential()\n",
    "\n",
    "beat_feature_model.add(Dense(500, input_dim=num_features_total, init='uniform'))\n",
    "beat_feature_model.add(BatchNormalization())\n",
    "beat_feature_model.add(Activation('tanh'))\n",
    "beat_feature_model.add(Dropout(0.5))\n",
    "\n",
    "beat_feature_model.add(Dense(500, init='uniform'))\n",
    "beat_feature_model.add(BatchNormalization())\n",
    "beat_feature_model.add(Activation('tanh'))\n",
    "beat_feature_model.add(Dropout(0.5))\n",
    "\n",
    "beat_feature_model.add(Dense(500, init='uniform'))\n",
    "beat_feature_model.add(BatchNormalization())\n",
    "beat_feature_model.add(Activation('tanh'))\n",
    "beat_feature_model.add(Dropout(0.5))\n",
    "\n",
    "#beat_feature_model.add(Dense(1, init='uniform'))\n",
    "beat_feature_model.add(Dense(num_classes, init='uniform'))\n",
    "beat_feature_model.add(BatchNormalization())\n",
    "beat_feature_model.add(Activation('softmax'))\n",
    "\n",
    "#beat_feature_model.compile(loss='mean_squared_error',\n",
    "beat_feature_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='adadelta',\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49430/49430 [==============================] - 53s - loss: 1.1318 - acc: 0.5764    \n",
      "Epoch 2/10\n",
      "49430/49430 [==============================] - 53s - loss: 0.8116 - acc: 0.6701    \n",
      "Epoch 3/10\n",
      "49430/49430 [==============================] - 53s - loss: 0.7661 - acc: 0.6784    \n",
      "Epoch 4/10\n",
      "49430/49430 [==============================] - 54s - loss: 0.7539 - acc: 0.6830    \n",
      "Epoch 5/10\n",
      "49430/49430 [==============================] - 55s - loss: 0.7483 - acc: 0.6836    \n",
      "Epoch 6/10\n",
      "49430/49430 [==============================] - 55s - loss: 0.7442 - acc: 0.6819    \n",
      "Epoch 7/10\n",
      "49430/49430 [==============================] - 57s - loss: 0.7409 - acc: 0.6829    \n",
      "Epoch 8/10\n",
      "49430/49430 [==============================] - 64s - loss: 0.7362 - acc: 0.6856    \n",
      "Epoch 9/10\n",
      "49430/49430 [==============================] - 47s - loss: 0.7355 - acc: 0.6855    \n",
      "Epoch 10/10\n",
      "49430/49430 [==============================] - 46s - loss: 0.7334 - acc: 0.6853    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1314f8390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = np.zeros((len(y_train), num_classes))\n",
    "y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "beat_feature_model.fit(np.array(X_train), np.array(y_one_hot), nb_epoch=7, batch_size=10) #, class_weight=class_weight)\n",
    "beat_feature_model.save('models/beat_importance_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "601265/601265 [==============================] - 218s - loss: 1.3761 - acc: 0.6271   \n",
      "Epoch 2/3\n",
      "601265/601265 [==============================] - 138s - loss: 0.9343 - acc: 0.8105   \n",
      "Epoch 3/3\n",
      "601265/601265 [==============================] - 138s - loss: 0.6548 - acc: 0.8506   \n"
     ]
    }
   ],
   "source": [
    "y_one_hot = np.zeros((len(y_train_full), num_classes))\n",
    "y_one_hot[np.arange(len(y_train_full)), y_train_full] = 1\n",
    "\n",
    "beat_feature_model.fit(np.array(X_train_full), np.array(y_one_hot), nb_epoch=3, batch_size=100) #, class_weight=class_weight)\n",
    "beat_feature_model.save('models/beat_importance_full_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beat_feature_model = load_model('models/beat_importance_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_importance(row):\n",
    "    return (1 - row[0]) * (row[1] + row[2] * 2 + row[3] * 30 + row[4] * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_song(key, clf):\n",
    "    song_X, song_y = get_features_for_song(key, True)\n",
    "    new_song_y = clf.predict(song_X)\n",
    "    beat_importance = [calculate_importance(row) for row in new_song_y]\n",
    "    \n",
    "    #print ('Length: ' + str(len(new_song_y)))\n",
    "    #plt.plot([new_song_y[i] for i in range(len(new_song_y)) if i % 12 == 0])\n",
    "    #plt.show()\n",
    "    \n",
    "    pd.DataFrame(beat_importance).to_csv('generated_data/{0}_importance_generated.csv'.format(key), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error loading song\n",
      "In The Groove~I Think I Like That Sound\n",
      "\n",
      "Error loading song\n",
      "In The Groove~Remember December\n",
      "\n",
      "Error loading song\n",
      "In The Groove~Torn\n",
      "\n",
      "Error loading song\n",
      "In The Groove~Walking on Fire\n",
      "\n",
      "Error loading song\n",
      "In The Groove 2~!\n"
     ]
    }
   ],
   "source": [
    "songs_to_use = pd.read_csv('data/songs_to_use.csv').values\n",
    "for song_data in songs_to_use:\n",
    "    try:\n",
    "        step_song(song_data[0], beat_feature_model)\n",
    "    except:\n",
    "        print ('\\nError loading song')\n",
    "        print (song_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
