{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from os import listdir\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import median, diff\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS\n",
    "- generate percent of single double notes etc with a nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "- 0: one note\n",
    "- 1: two notes\n",
    "- 2: three or four notes\n",
    "- 3: hold start\n",
    "- 4: roll start\n",
    "- 5: mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "songs_to_use_full = pd.read_csv('data/songs_to_use.csv').values\n",
    "save_files = listdir('data')\n",
    "songs_to_use = [song_data for song_data in songs_to_use_full if '{0}_misc.csv'.format(song_data[0]) in save_files]\n",
    "songs = [SongFile(song_data[0]) for song_data in songs_to_use]\n",
    "np.random.shuffle(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_for_index_expanded(notes, index):\n",
    "    if index < 0:\n",
    "        return [0, 0, 0, 0, 0, 0]\n",
    "    row = notes[index][0]\n",
    "    (steps, holds, rolls, mines) = [row.count(char) for char in ['1', '2', '4', 'M']]\n",
    "    if steps == 0 and mines == 0 and holds == 0 and rolls == 0:\n",
    "        return [0, 0, 0, 0, 0, 0]\n",
    "    steps += (holds + rolls)\n",
    "    return [int(i) for i in [steps == 1, steps == 2, steps > 2, holds > 0, rolls > 0, mines > 0]]\n",
    "\n",
    "def get_class_for_index(notes, index):\n",
    "    classes_expanded = get_class_for_index_expanded(notes, index)\n",
    "    return [i for i in range(6) if classes_expanded[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_per_bar = 48\n",
    "class SongFile:\n",
    "    def __init__(self, key):\n",
    "        misc = pd.read_csv('data/{0}_misc.csv'.format(key)).values\n",
    "        self.bpm = misc[1][0]\n",
    "        self.notes = pd.read_csv('data/{0}_notes.csv'.format(key), converters={'0': lambda x: str(x)}).values\n",
    "        self.note_classes = [get_class_for_index_expanded(self.notes, i) for i in range(len(self.notes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beats_to_track = 48\n",
    "note_types = ['1', 'M', '2', '4', '3']\n",
    "\n",
    "def get_features_for_row(row):\n",
    "    return [int(char == target) for target in note_types for char in row]\n",
    "\n",
    "empty_row = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def get_previous_notes(index, features):\n",
    "    previous_notes = [features[i] for i in range(index, index + song_padding) if not np.array_equal(features[i], empty_row)]\n",
    "    return [empty_row] * (8 - len(previous_notes)) + previous_notes[-8:]\n",
    "    \n",
    "song_padding = beats_to_track * 2\n",
    "song_end_padding = beats_to_track * 2\n",
    "important_indices = [1, 2, 3, 4, 8, 16, 20, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96]\n",
    "important_indices_classes = [-96, -84, -72, -60, -48, -36, -24, -12, 0, 1, 2, 3, 4, 8, 16, 20, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96]\n",
    "def get_features(index, features, note_classes):\n",
    "    indices = [index + song_padding - i for i in important_indices]\n",
    "    indices_classes = [index + song_padding - i for i in important_indices_classes]\n",
    "    past_classes = np.array([note_classes[i] for i in indices_classes]).flatten()\n",
    "    past_features = np.array([features[i] for i in indices]).flatten()\n",
    "    previous_notes = np.array(get_previous_notes(index, features)).flatten()\n",
    "    return np.concatenate((past_classes, past_features, previous_notes), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beats_to_track = 48\n",
    "note_types = ['1', 'M', '2', '4', '3']\n",
    "\n",
    "def get_features_for_row(row):\n",
    "    return [int(char == target) for target in note_types for char in row]\n",
    "\n",
    "feature_important_indices = [1, 2, 3, 4, 6, 8, 12, 16, 18, 20, 24, 30, 36, 48, 60, 72, 84, 96]\n",
    "note_important_indices = [-48, -24, -12, -6, -3, 0, 3, 4, 6, 8, 12, 16, 18, 24, 36, 48, 72, 96]\n",
    "\n",
    "prev_note_length = len(important_indices)\n",
    "empty_row = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def get_previous_notes(index, features):\n",
    "    previous_notes = [features[index + song_padding - i] for i in range(1, 96) if not np.array_equal(features[i], empty_row)]\n",
    "    return [empty_row] * (prev_note_length - len(previous_notes)) + previous_notes[-prev_note_length:]\n",
    "\n",
    "song_padding = 96\n",
    "song_end_padding = 48\n",
    "\n",
    "def get_features(index, features, note_classes):\n",
    "    past_notes = get_previous_notes(index, features)\n",
    "    feature_indices = [index + song_padding - i for i in feature_important_indices]\n",
    "    note_indices = [index + song_padding - i for i in note_important_indices]\n",
    "    return [np.concatenate((past_notes[i], features[feature_indices[i]], note_classes[note_indices[i]]), axis = 0) for i in range(prev_note_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_output_for_class(model_class, row):\n",
    "    if model_class == 0 or model_class == 1 or model_class == 2:\n",
    "        return [int(char == '1' or char == '2' or char == '4') for char in row]\n",
    "    if model_class == 3:\n",
    "        return [int(char == '2') for char in row]\n",
    "    if model_class == 4:\n",
    "        return [int(char == '4') for char in row]\n",
    "    if model_class == 5:\n",
    "        return [int(char == 'M') for char in row]\n",
    "\n",
    "def get_hold_length(notes, note_row, note_column):\n",
    "    i = 0\n",
    "    while i < len(notes) - note_row:\n",
    "        if notes[note_row + i][0][note_column] == '3':\n",
    "            return i\n",
    "        i += 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features_for_songs(songs):\n",
    "    hold_X = []\n",
    "    roll_X = []\n",
    "    hold_y = []\n",
    "    roll_y = []\n",
    "    X = [[] for i in range(6)]\n",
    "    y = [[] for i in range(6)]\n",
    "    for song in songs:\n",
    "        note_classes = np.concatenate((([[0, 0, 0, 0, 0, 0]] * song_padding), song.note_classes, ([[0, 0, 0, 0, 0, 0]] * song_end_padding)), axis = 0)\n",
    "        notes = np.concatenate((([['0000']] * song_padding), song.notes), axis = 0)\n",
    "        if abs(len(note_classes) - len(notes) > 250):\n",
    "            print ('Lengths dont match for {0}'.format(key))\n",
    "            print ('{0} vs {1}'.format(len(note_classes), len(notes)))\n",
    "            continue\n",
    "        length = min(len(note_classes) - song_padding - song_end_padding, len(notes) - song_padding)\n",
    "        features = np.array([get_features_for_row(notes[i][0]) for i in range(0, length + song_padding)])\n",
    "        for i in range(length):\n",
    "            row = notes[i + song_padding][0]\n",
    "            model_classes = get_class_for_index(notes, i + song_padding)\n",
    "            for model_class in model_classes:\n",
    "                X_row = get_features(i, features, note_classes)\n",
    "                X[model_class].append(X_row)\n",
    "                y[model_class].append(get_model_output_for_class(model_class, row))\n",
    "                \n",
    "                if model_class == 3:\n",
    "                    for j in range(4):\n",
    "                        if row[j] == '2':\n",
    "                            length = get_hold_length(notes, i + song_padding, j)\n",
    "                            if length:\n",
    "                                hold_X.append(X_row)\n",
    "                                hold_y.append(length)\n",
    "                if model_class == 4:\n",
    "                    for j in range(4):\n",
    "                        if row[j] == '4':\n",
    "                            length = get_hold_length(notes, i + song_padding, j)\n",
    "                            if length:\n",
    "                                roll_X.append(X_row)\n",
    "                                roll_y.append(length)\n",
    "\n",
    "    X = [np.array(X_for_class) for X_for_class in X]\n",
    "    y = [np.array(y_for_class) for y_for_class in y]\n",
    "    return X, y, np.array(hold_X), np.array(hold_y), np.array(roll_X), np.array(roll_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 25s, sys: 3.36 s, total: 2min 28s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_array, y_train_array, hold_X_train, hold_y_train, roll_X_train, roll_y_train = get_features_for_songs(songs[:174]) # total 217\n",
    "X_test_array, y_test_array, hold_X_test, hold_y_test, roll_X_test, roll_y_test = get_features_for_songs(songs[174:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_lengths = [3, 6, 9, 12, 18, 24, 36, 48]\n",
    "def get_closest_hold_lengths(lengths):\n",
    "    closest = [np.argmax([-abs(length - aprox) for aprox in hold_lengths]) for length in lengths]\n",
    "    closest_one_hot = np.zeros((len(closest), len(hold_lengths)))\n",
    "    closest_one_hot[np.arange(len(closest)), closest] = 1\n",
    "    return np.array(closest_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hold_y_train = get_closest_hold_lengths(hold_y_train)\n",
    "roll_y_train = get_closest_hold_lengths(roll_y_train)\n",
    "hold_y_test = get_closest_hold_lengths(hold_y_test)\n",
    "roll_y_test = get_closest_hold_lengths(roll_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(128, batch_input_shape=[64, 18, 46], stateful=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softsign'))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adagrad',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_array[0]\n",
    "y_train = y_train_array[0]\n",
    "X_test = X_test_array[0]\n",
    "y_test = y_test_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78784 samples, validate on 18432 samples\n",
      "Epoch 1/8\n",
      "78784/78784 [==============================] - 74s - loss: 1.3649 - acc: 0.3185 - val_loss: 1.3820 - val_acc: 0.3128\n",
      "Epoch 2/8\n",
      "78784/78784 [==============================] - 66s - loss: 1.3399 - acc: 0.3516 - val_loss: 1.3447 - val_acc: 0.3503\n",
      "Epoch 3/8\n",
      "78784/78784 [==============================] - 64s - loss: 1.3271 - acc: 0.3610 - val_loss: 1.3362 - val_acc: 0.3544\n",
      "Epoch 4/8\n",
      "78784/78784 [==============================] - 96s - loss: 1.3168 - acc: 0.3663 - val_loss: 1.3312 - val_acc: 0.3580\n",
      "Epoch 5/8\n",
      "78784/78784 [==============================] - 179s - loss: 1.3097 - acc: 0.3713 - val_loss: 1.3196 - val_acc: 0.3632\n",
      "Epoch 6/8\n",
      "78784/78784 [==============================] - 68s - loss: 1.3026 - acc: 0.3756 - val_loss: 1.3160 - val_acc: 0.3670\n",
      "Epoch 7/8\n",
      "78784/78784 [==============================] - 66s - loss: 1.2962 - acc: 0.3786 - val_loss: 1.3172 - val_acc: 0.3639\n",
      "Epoch 8/8\n",
      "78784/78784 [==============================] - 66s - loss: 1.2903 - acc: 0.3801 - val_loss: 1.3105 - val_acc: 0.3665\n",
      "18432/18432 [==============================] - 4s     \n",
      "[1.3106080107390881, 0.36631944444444442]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, batch_input_shape=[64, 18, 46], stateful=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softsign'))\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "train_cutoff = int(len(X_train) / batch_size) * batch_size\n",
    "test_cutoff = int(len(X_test) / batch_size) * batch_size\n",
    "model.fit(X_train[:train_cutoff], y_train[:train_cutoff], nb_epoch=8, batch_size=batch_size, verbose=1, validation_data=(X_test[:test_cutoff], y_test[:test_cutoff]))\n",
    "print (model.evaluate(X_test[:test_cutoff], y_test[:test_cutoff], batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_flat = np.array([np.array(row).flatten() for row in X_train])\n",
    "X_test_flat = np.array([np.array(row).flatten() for row in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78784 samples, validate on 18432 samples\n",
      "Epoch 1/8\n",
      "78784/78784 [==============================] - 21s - loss: 1.2886 - acc: 0.3589 - val_loss: 1.2464 - val_acc: 0.3748\n",
      "Epoch 2/8\n",
      "78784/78784 [==============================] - 21s - loss: 1.2283 - acc: 0.3983 - val_loss: 1.2309 - val_acc: 0.3855\n",
      "Epoch 3/8\n",
      "78784/78784 [==============================] - 21s - loss: 1.2082 - acc: 0.4094 - val_loss: 1.2267 - val_acc: 0.3873\n",
      "Epoch 4/8\n",
      "78784/78784 [==============================] - 22s - loss: 1.1960 - acc: 0.4208 - val_loss: 1.2214 - val_acc: 0.3895\n",
      "Epoch 5/8\n",
      "78784/78784 [==============================] - 22s - loss: 1.1836 - acc: 0.4309 - val_loss: 1.2201 - val_acc: 0.3928\n",
      "Epoch 6/8\n",
      "78784/78784 [==============================] - 21s - loss: 1.1734 - acc: 0.4387 - val_loss: 1.2193 - val_acc: 0.3940\n",
      "Epoch 7/8\n",
      "78784/78784 [==============================] - 22s - loss: 1.1668 - acc: 0.4429 - val_loss: 1.2186 - val_acc: 0.3943\n",
      "Epoch 8/8\n",
      "78784/78784 [==============================] - 22s - loss: 1.1593 - acc: 0.4496 - val_loss: 1.2184 - val_acc: 0.3965\n",
      "18368/18432 [============================>.] - ETA: 0s[1.2183941880034075, 0.396484375]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(828,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "train_cutoff = int(len(X_train) / batch_size) * batch_size\n",
    "test_cutoff = int(len(X_test) / batch_size) * batch_size\n",
    "model.fit(X_train_flat[:train_cutoff], y_train[:train_cutoff], nb_epoch=8, batch_size=batch_size, verbose=1, validation_data=(X_test_flat[:test_cutoff], y_test[:test_cutoff]))\n",
    "print (model.evaluate(X_test_flat[:test_cutoff], y_test[:test_cutoff], batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1629 samples\n",
      "Epoch 1/5\n",
      "6985/6985 [==============================] - 4s - loss: 1.5991 - acc: 0.4777 - val_loss: 1.8146 - val_acc: 0.3806\n",
      "Epoch 2/5\n",
      "6985/6985 [==============================] - 5s - loss: 1.1324 - acc: 0.6759 - val_loss: 1.7726 - val_acc: 0.3677\n",
      "Epoch 3/5\n",
      "6985/6985 [==============================] - 5s - loss: 0.8831 - acc: 0.7801 - val_loss: 1.7566 - val_acc: 0.3800\n",
      "Epoch 4/5\n",
      "6985/6985 [==============================] - 4s - loss: 0.6957 - acc: 0.8624 - val_loss: 1.9463 - val_acc: 0.3143\n",
      "Epoch 5/5\n",
      "6985/6985 [==============================] - 5s - loss: 0.5773 - acc: 0.9138 - val_loss: 1.8850 - val_acc: 0.3554\n"
     ]
    }
   ],
   "source": [
    "model = build_model(len(hold_lengths))\n",
    "model.fit(hold_X_train, hold_y_train, nb_epoch=5, batch_size=64, verbose=1, validation_data=(hold_X_test, hold_y_test))\n",
    "model.save('models/hold_length_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 195 samples, validate on 67 samples\n",
      "Epoch 1/5\n",
      "195/195 [==============================] - 0s - loss: 1.8828 - acc: 0.3333 - val_loss: 3.0420 - val_acc: 0.1045\n",
      "Epoch 2/5\n",
      "195/195 [==============================] - 0s - loss: 0.7856 - acc: 0.8923 - val_loss: 2.1661 - val_acc: 0.0896\n",
      "Epoch 3/5\n",
      "195/195 [==============================] - 0s - loss: 0.6140 - acc: 0.9538 - val_loss: 2.1022 - val_acc: 0.0896\n",
      "Epoch 4/5\n",
      "195/195 [==============================] - 0s - loss: 0.5359 - acc: 0.9692 - val_loss: 2.2033 - val_acc: 0.0746\n",
      "Epoch 5/5\n",
      "195/195 [==============================] - 0s - loss: 0.5048 - acc: 0.9846 - val_loss: 2.1390 - val_acc: 0.0746\n"
     ]
    }
   ],
   "source": [
    "model = build_model(len(hold_lengths))\n",
    "model.fit(roll_X_train, roll_y_train, nb_epoch=5, batch_size=64, verbose=1, validation_data=(roll_X_test, roll_y_test))\n",
    "model.save('models/roll_length_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_arrays = [\n",
    "    ['1000', '0100', '0010', '0001'],\n",
    "    ['1100', '1010', '1001', '0110', '0101', '0011'],\n",
    "    ['1110', '1101', '1011', '0111', '1111'],\n",
    "    ['1000', '0100', '0010', '0001', '2', '3', '4'],\n",
    "    ['1000', '0100', '0010', '0001', '2', '3', '4'],\n",
    "    ['1000', '0100', '0010', '0001', '2', '3', '4'],\n",
    "]\n",
    "class_maps = [dict((class_array[i], i) for i in range(len(class_array))) for class_array in class_arrays]\n",
    "def get_class(class_map, y_row):\n",
    "    as_string = ''.join(str(x) for x in y_row)\n",
    "    pos_count = as_string.count('1')\n",
    "    return class_map[str(pos_count)] if '2' in class_map and pos_count > 1 else class_map[as_string]\n",
    "\n",
    "def get_y_not_one_hot(y):\n",
    "    return [[get_class(class_map, y_row) for y_row in y_section] for class_map, y_section in zip(class_maps, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_classes = get_y_not_one_hot(y_train_array)\n",
    "y_test_classes = get_y_not_one_hot(y_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 48s, sys: 3.33 s, total: 5min 51s\n",
      "Wall time: 6min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_samples_leafs = [16, 4, 1, 4, 1, 2]\n",
    "clf = RandomForestClassifier(n_estimators = 50, min_samples_leaf=16, max_features=600)\n",
    "clf.fit(X_train_flat, y_train)\n",
    "clf.score(X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10965506037797151"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.6588750160895869\n",
      "Testing: 0.4779569343810593\n",
      "Training: 0.9802076573653472\n",
      "Testing: 0.2541047693510555\n",
      "Training: 1.0\n",
      "Testing: 0.16666666666666666\n",
      "Training: 0.9415430267062315\n",
      "Testing: 0.43221476510067114\n",
      "Training: 1.0\n",
      "Testing: 0.2878787878787879\n",
      "Training: 0.9955837703560585\n",
      "Testing: 0.3305785123966942\n",
      "CPU times: user 5min 41s, sys: 1.11 s, total: 5min 42s\n",
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_samples_leafs = [16, 4, 1, 4, 1, 2]\n",
    "for X_train, y_train, X_test, y_test, min_samples_leaf in zip(X_train_array, y_train_classes, X_test_array, y_test_classes, min_samples_leafs):\n",
    "    clf = RandomForestClassifier(n_estimators = 50, min_samples_leaf=min_samples_leaf, max_features=600)\n",
    "    test(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.42578195391942336\n",
      "Testing: 0.4110113276864986\n",
      "Training: 0.3238157040882544\n",
      "Testing: 0.17982799061767005\n",
      "Training: 1.0\n",
      "Testing: 0.0\n",
      "Training: 0.4986646884272997\n",
      "Testing: 0.3825503355704698\n",
      "Training: 1.0\n",
      "Testing: 0.3333333333333333\n",
      "Training: 0.6213083080320176\n",
      "Testing: 0.2833530106257379\n",
      "CPU times: user 18.6 s, sys: 590 ms, total: 19.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for X_train, y_train, X_test, y_test in zip(X_train_array, y_train_classes, X_test_array, y_test_classes):\n",
    "    clf = SGDClassifier(loss=\"log\", n_iter=20)\n",
    "    test(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.7332861372119964\n",
      "Testing: 0.4669864271864476\n",
      "Training: 0.9928617780661908\n",
      "Testing: 0.22830336200156373\n",
      "Training: 0.22727272727272727\n",
      "Testing: 0.3333333333333333\n",
      "Training: 0.9899109792284867\n",
      "Testing: 0.4161073825503356\n",
      "Training: 0.8823529411764706\n",
      "Testing: 0.16666666666666666\n",
      "Training: 0.9895114545956389\n",
      "Testing: 0.32585596221959856\n",
      "CPU times: user 39min 48s, sys: 19.3 s, total: 40min 8s\n",
      "Wall time: 11min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for X_train, y_train, X_test, y_test in zip(X_train_array, y_train_classes, X_test_array, y_test_classes):\n",
    "    clf = XGBClassifier(max_depth=10, min_child_weight=8, learning_rate=0.3, seed=0, n_estimators=100, subsample=0.80, colsample_bytree=0.80, objective=\"multi:softprob\")\n",
    "    test(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.6738318959969108\n",
      "Testing: 0.47652821716501687\n",
      "Training: 0.9446787800129786\n",
      "Testing: 0.21266614542611414\n",
      "Training: 1.0\n",
      "Testing: 0.0\n",
      "Training: 0.9256676557863501\n",
      "Testing: 0.43355704697986575\n",
      "Training: 1.0\n",
      "Testing: 0.21212121212121213\n",
      "Training: 0.962186033673751\n",
      "Testing: 0.31286894923258557\n",
      "CPU times: user 46min 59s, sys: 29.5 s, total: 47min 29s\n",
      "Wall time: 20min 1s\n",
      "CPU times: user 46min 59s, sys: 29.5 s, total: 47min 29s\n",
      "Wall time: 20min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_samples_leafs = [16, 4, 1, 4, 1, 2]\n",
    "for X_train, y_train, X_test, y_test, min_samples_leaf, i in zip(X_train_array, y_train_classes, X_test_array, y_test_classes, min_samples_leafs, range(6)):\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 50, min_samples_leaf=min_samples_leaf, max_features=600)\n",
    "    sgd_clf = SGDClassifier(loss=\"log\", n_iter=20)\n",
    "    xgb_clf = XGBClassifier(max_depth=10, min_child_weight=8, learning_rate=0.3, seed=0, n_estimators=100, subsample=0.80, colsample_bytree=0.80, objective=\"multi:softprob\")\n",
    "    eclf = EnsembleVoteClassifier(clfs=[xgb_clf, rf_clf, sgd_clf], weights=[1, 1, 1], voting='soft')\n",
    "    test(eclf)\n",
    "    joblib.dump(eclf, 'models/note_class_eclf/clf_{0}.pkl'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.6310722100656455\n",
      "Testing: 0.45096438412234935\n",
      "Training: 0.5314730693967523\n",
      "Testing: 0.2947615321443079\n",
      "Training: 0.2727272727272727\n",
      "Testing: 0.0\n",
      "Training: 0.9320474777448071\n",
      "Testing: 0.352348993293591\n",
      "Training: 0.967914441690088\n",
      "Testing: 0.3181818181818182\n",
      "Training: 0.7728401876897598\n",
      "Testing: 0.3069657615464018\n",
      "CPU times: user 13min 40s, sys: 23.3 s, total: 14min 3s\n",
      "Wall time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_model(num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512, input_shape=(812,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adagrad',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "batch_sizes = [64, 16, 2, 16, 4, 8]\n",
    "models = []\n",
    "for X_train, y_train, X_test, y_test, batch_size in zip(X_train_array, y_train_array, X_test_array, y_test_array, batch_sizes):\n",
    "    model = build_model(4)\n",
    "    model.fit(X_train, y_train, nb_epoch=15, batch_size=batch_size, verbose=0, validation_data=(X_test, y_test))\n",
    "    print ('Training: {0}'.format(model.evaluate(X_train, y_train, verbose=0)[1]))\n",
    "    print ('Testing: {0}'.format(model.evaluate(X_test, y_test, verbose=0)[1]))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.array([1, 2]), np.array([3, 4])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
