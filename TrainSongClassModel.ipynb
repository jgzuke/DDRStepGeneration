{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note types\n",
    "- 0: nothing\n",
    "- 1: step\n",
    "- 2: hold start\n",
    "- 3: hold/roll end\n",
    "- 4: roll start\n",
    "- M: mine\n",
    "\n",
    "# Classes\n",
    "- 0: nothing\n",
    "- 1: one note\n",
    "- 2: two notes\n",
    "- 3: three or four notes\n",
    "- 4: hold start\n",
    "- 5: roll start\n",
    "- 6: mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_back_included = 8\n",
    "num_classes = 7\n",
    "num_features = 40\n",
    "num_features_total = (num_features * samples_back_included) + 4\n",
    "save_files = listdir('data')\n",
    "\n",
    "def get_features_for_index(beat_features, notes, index):\n",
    "    if index < 0:\n",
    "        return [0] * num_features\n",
    "    return beat_features[index]\n",
    "\n",
    "def get_class_for_index(notes, index):\n",
    "    if index < 0:\n",
    "        return [1, 0, 0, 0, 0, 0, 0]\n",
    "    row = notes[index][0]\n",
    "    (steps, holds, rolls, mines) = [row.count(char) for char in ['1', '2', '4', 'M']]\n",
    "    steps += (holds + rolls)\n",
    "    return [int(i) for i in [steps == 0 and mines == 0, steps == 1, steps == 2, steps > 2, holds > 0, rolls > 0, mines > 0]]\n",
    "    \n",
    "importance_rankings = [48, 24, 12, 16, 6, 8, 3, 4, 2, 1]\n",
    "def get_beat_importance(index):\n",
    "    for i in range(len(importance_rankings)):\n",
    "        if index % importance_rankings[i] == 0:\n",
    "            return i\n",
    "\n",
    "def get_features_for_song(key, is_full):\n",
    "    X = []\n",
    "    y = []\n",
    "    if '{0}_beat_features.csv'.format(key) in save_files and '{0}_notes.csv'.format(key) in save_files:\n",
    "        beat_features_rotated = pd.read_csv('data/{0}_beat_features.csv'.format(key)).values\n",
    "        notes = pd.read_csv('data/{0}_notes.csv'.format(key), converters={'0': lambda x: str(x)}).values\n",
    "        beat_features = np.flipud(np.rot90(np.array(beat_features_rotated)))\n",
    "        num_notes = min(len(notes), len(beat_features))\n",
    "        for i in range(num_notes):\n",
    "            row_y = get_class_for_index(notes, i)\n",
    "            if is_full or (not (row_y == 0 and random.randint(0, 5) != 0)):\n",
    "                features = [feature for j in range(samples_back_included) for feature in get_features_for_index(beat_features, notes, i - j)]\n",
    "                features.append(i % 48)\n",
    "                features.append(get_beat_importance(i))\n",
    "                features.append(i / 48)\n",
    "                features.append(num_notes - i / 48)\n",
    "                X.append(features)\n",
    "                y.append(row_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Total 243 songs\n",
    "def build_training_data(songs_start, songs_end, is_full = False):\n",
    "    X = []\n",
    "    y = []\n",
    "    songs_to_use = pd.read_csv('data/songs_to_use.csv').values\n",
    "    for song_data in songs_to_use[songs_start:songs_end]:\n",
    "        song_X, song_y = get_features_for_song(song_data[0], is_full)\n",
    "        X.extend(song_X)\n",
    "        y.extend(song_y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = build_training_data(0, 243, True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X = None\n",
    "y = None\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = {\n",
    "    0: 1,\n",
    "    1: 2,\n",
    "    2: 4,\n",
    "    3: 8,\n",
    "    4: 4,\n",
    "    5: 4,\n",
    "    6: 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_test(beat_feature_model):\n",
    "    print ('Train Error')\n",
    "    print (beat_feature_model.evaluate(X_train, y_train, batch_size=64))\n",
    "    print ('Test Error')\n",
    "    print (beat_feature_model.evaluate(X_test, y_test, batch_size=64))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 481012 samples, validate on 120253 samples\n",
      "Epoch 1/50\n",
      "481012/481012 [==============================] - 146s - loss: 0.8557 - acc: 0.8430 - val_loss: 0.5917 - val_acc: 0.8689\n",
      "Epoch 2/50\n",
      "481012/481012 [==============================] - 134s - loss: 0.5401 - acc: 0.8700 - val_loss: 0.4977 - val_acc: 0.8712\n",
      "Epoch 3/50\n",
      "481012/481012 [==============================] - 142s - loss: 0.4769 - acc: 0.8725 - val_loss: 0.4514 - val_acc: 0.8753\n",
      "Epoch 4/50\n",
      "481012/481012 [==============================] - 124s - loss: 0.4498 - acc: 0.8736 - val_loss: 0.4335 - val_acc: 0.8761\n",
      "Epoch 5/50\n",
      "481012/481012 [==============================] - 133s - loss: 0.4347 - acc: 0.8746 - val_loss: 0.4222 - val_acc: 0.8751\n",
      "Epoch 6/50\n",
      "481012/481012 [==============================] - 129s - loss: 0.4240 - acc: 0.8750 - val_loss: 0.4138 - val_acc: 0.8785\n",
      "Epoch 7/50\n",
      "481012/481012 [==============================] - 136s - loss: 0.4171 - acc: 0.8755 - val_loss: 0.4072 - val_acc: 0.8786\n",
      "Epoch 8/50\n",
      "481012/481012 [==============================] - 130s - loss: 0.4113 - acc: 0.8762 - val_loss: 0.4043 - val_acc: 0.8756\n",
      "Epoch 9/50\n",
      "481012/481012 [==============================] - 130s - loss: 0.4070 - acc: 0.8768 - val_loss: 0.4047 - val_acc: 0.8796\n",
      "Epoch 10/50\n",
      "481012/481012 [==============================] - 127s - loss: 0.4033 - acc: 0.8775 - val_loss: 0.3952 - val_acc: 0.8794\n",
      "Epoch 11/50\n",
      "481012/481012 [==============================] - 125s - loss: 0.4003 - acc: 0.8781 - val_loss: 0.3920 - val_acc: 0.8805\n",
      "Epoch 12/50\n",
      "481012/481012 [==============================] - 141s - loss: 0.3970 - acc: 0.8783 - val_loss: 0.3916 - val_acc: 0.8778\n",
      "Epoch 13/50\n",
      "481012/481012 [==============================] - 144s - loss: 0.3945 - acc: 0.8786 - val_loss: 0.3926 - val_acc: 0.8814\n",
      "Epoch 14/50\n",
      "481012/481012 [==============================] - 129s - loss: 0.3924 - acc: 0.8788 - val_loss: 0.3893 - val_acc: 0.8810\n",
      "Epoch 15/50\n",
      "481012/481012 [==============================] - 140s - loss: 0.3904 - acc: 0.8793 - val_loss: 0.3840 - val_acc: 0.8801\n",
      "Epoch 16/50\n",
      "481012/481012 [==============================] - 173s - loss: 0.3887 - acc: 0.8798 - val_loss: 0.3858 - val_acc: 0.8778\n",
      "Epoch 17/50\n",
      "481012/481012 [==============================] - 143s - loss: 0.3869 - acc: 0.8797 - val_loss: 0.3812 - val_acc: 0.8819\n",
      "Epoch 18/50\n",
      "481012/481012 [==============================] - 137s - loss: 0.3846 - acc: 0.8804 - val_loss: 0.3824 - val_acc: 0.8826\n",
      "Epoch 19/50\n",
      "481012/481012 [==============================] - 142s - loss: 0.3838 - acc: 0.8810 - val_loss: 0.3814 - val_acc: 0.8825\n",
      "Epoch 20/50\n",
      "481012/481012 [==============================] - 148s - loss: 0.3820 - acc: 0.8813 - val_loss: 0.3791 - val_acc: 0.8788\n",
      "Epoch 21/50\n",
      "481012/481012 [==============================] - 143s - loss: 0.3808 - acc: 0.8815 - val_loss: 0.3754 - val_acc: 0.8836\n",
      "Epoch 22/50\n",
      "481012/481012 [==============================] - 134s - loss: 0.3796 - acc: 0.8816 - val_loss: 0.3729 - val_acc: 0.8844\n",
      "Epoch 23/50\n",
      "481012/481012 [==============================] - 134s - loss: 0.3782 - acc: 0.8818 - val_loss: 0.3784 - val_acc: 0.8793\n",
      "Epoch 24/50\n",
      "481012/481012 [==============================] - 139s - loss: 0.3771 - acc: 0.8823 - val_loss: 0.3739 - val_acc: 0.8837\n",
      "Epoch 25/50\n",
      "481012/481012 [==============================] - 128s - loss: 0.3758 - acc: 0.8829 - val_loss: 0.3703 - val_acc: 0.8842\n",
      "Epoch 26/50\n",
      "481012/481012 [==============================] - 124s - loss: 0.3745 - acc: 0.8827 - val_loss: 0.3708 - val_acc: 0.8829\n",
      "Epoch 27/50\n",
      "481012/481012 [==============================] - 123s - loss: 0.3738 - acc: 0.8831 - val_loss: 0.3699 - val_acc: 0.8842\n",
      "Epoch 28/50\n",
      "481012/481012 [==============================] - 123s - loss: 0.3722 - acc: 0.8831 - val_loss: 0.3688 - val_acc: 0.8850\n",
      "Epoch 29/50\n",
      "481012/481012 [==============================] - 125s - loss: 0.3719 - acc: 0.8834 - val_loss: 0.3671 - val_acc: 0.8851\n",
      "Epoch 30/50\n",
      "481012/481012 [==============================] - 129s - loss: 0.3703 - acc: 0.8839 - val_loss: 0.3666 - val_acc: 0.8848\n",
      "Epoch 31/50\n",
      "481012/481012 [==============================] - 136s - loss: 0.3694 - acc: 0.8841 - val_loss: 0.3657 - val_acc: 0.8848\n",
      "Epoch 32/50\n",
      "481012/481012 [==============================] - 140s - loss: 0.3692 - acc: 0.8839 - val_loss: 0.3661 - val_acc: 0.8851\n",
      "Epoch 33/50\n",
      "481012/481012 [==============================] - 142s - loss: 0.3680 - acc: 0.8841 - val_loss: 0.3679 - val_acc: 0.8827\n",
      "Epoch 34/50\n",
      "481012/481012 [==============================] - 141s - loss: 0.3668 - acc: 0.8846 - val_loss: 0.3644 - val_acc: 0.8859\n",
      "Epoch 35/50\n",
      "481012/481012 [==============================] - 138s - loss: 0.3661 - acc: 0.8847 - val_loss: 0.3632 - val_acc: 0.8859\n",
      "Epoch 36/50\n",
      "481012/481012 [==============================] - 137s - loss: 0.3655 - acc: 0.8849 - val_loss: 0.3627 - val_acc: 0.8863\n",
      "Epoch 37/50\n",
      "481012/481012 [==============================] - 134s - loss: 0.3648 - acc: 0.8853 - val_loss: 0.3616 - val_acc: 0.8865\n",
      "Epoch 38/50\n",
      "481012/481012 [==============================] - 132s - loss: 0.3642 - acc: 0.8855 - val_loss: 0.3611 - val_acc: 0.8866\n",
      "Epoch 39/50\n",
      "481012/481012 [==============================] - 132s - loss: 0.3636 - acc: 0.8850 - val_loss: 0.3603 - val_acc: 0.8860\n",
      "Epoch 40/50\n",
      "481012/481012 [==============================] - 131s - loss: 0.3620 - acc: 0.8860 - val_loss: 0.3600 - val_acc: 0.8864\n",
      "Epoch 41/50\n",
      "481012/481012 [==============================] - 125s - loss: 0.3618 - acc: 0.8856 - val_loss: 0.3595 - val_acc: 0.8868\n",
      "Epoch 42/50\n",
      "481012/481012 [==============================] - 126s - loss: 0.3606 - acc: 0.8863 - val_loss: 0.3603 - val_acc: 0.8855\n",
      "Epoch 43/50\n",
      "481012/481012 [==============================] - 123s - loss: 0.3606 - acc: 0.8861 - val_loss: 0.3590 - val_acc: 0.8872\n",
      "Epoch 44/50\n",
      "481012/481012 [==============================] - 125s - loss: 0.3600 - acc: 0.8863 - val_loss: 0.3585 - val_acc: 0.8873\n",
      "Epoch 45/50\n",
      "481012/481012 [==============================] - 123s - loss: 0.3582 - acc: 0.8865 - val_loss: 0.3579 - val_acc: 0.8874\n",
      "Epoch 46/50\n",
      "481012/481012 [==============================] - 122s - loss: 0.3582 - acc: 0.8868 - val_loss: 0.3588 - val_acc: 0.8872\n",
      "Epoch 47/50\n",
      "481012/481012 [==============================] - 122s - loss: 0.3577 - acc: 0.8869 - val_loss: 0.3563 - val_acc: 0.8872\n",
      "Epoch 48/50\n",
      "481012/481012 [==============================] - 120s - loss: 0.3566 - acc: 0.8872 - val_loss: 0.3554 - val_acc: 0.8876\n",
      "Epoch 49/50\n",
      "481012/481012 [==============================] - 120s - loss: 0.3562 - acc: 0.8873 - val_loss: 0.3550 - val_acc: 0.8878\n",
      "Epoch 50/50\n",
      "481012/481012 [==============================] - 120s - loss: 0.3558 - acc: 0.8878 - val_loss: 0.3562 - val_acc: 0.8872\n",
      "Train Error\n",
      "481012/481012 [==============================] - 32s    \n",
      "[0.33207824810057229, 0.89525209350187351]\n",
      "Test Error\n",
      "120192/120253 [============================>.] - ETA: 0s[0.35618546014481289, 0.88724605623094976]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121183"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(324,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, nb_epoch=50, batch_size=64, verbose=1, validation_data=(X_test, y_test))\n",
    "train_and_test(model)\n",
    "model.save('models/song_class_model_alt.h5')\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 481012 samples, validate on 120253 samples\n",
      "Epoch 1/50\n",
      "447168/481012 [==========================>...] - ETA: 8s - loss: 1.2282 - acc: 0.8046"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(324,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, nb_epoch=50, batch_size=64, verbose=1, class_weight=class_weight, validation_data=(X_test, y_test))\n",
    "train_and_test(model)\n",
    "model.save('models/song_class_model_weighted.h5')\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
